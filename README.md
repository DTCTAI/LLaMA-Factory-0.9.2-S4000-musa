# ğŸš€ LLaMA-Factory-S4000-MUSA

<p align="center">
  <img src="./assets/dtct.png" alt="æµ™æ±Ÿå¾·å¡”æ£®ç‰¹æ€»éƒ¨ç ”å‘ä¸­å¿ƒ" width="200"/>
</p>

<p align="center">
  <em>æµ™æ±Ÿå¾·å¡”æ£®ç‰¹æ€»éƒ¨ç ”å‘ä¸­å¿ƒ</em>
</p>

<p align="center">
  <a href="./LICENSE"><img src="https://img.shields.io/github/license/xxxx/LLaMA-Factory-S4000-MUSA" alt="License"/></a>
  <img src="https://img.shields.io/badge/python-3.10%2B-blue" alt="Python"/>
  <img src="https://img.shields.io/badge/torch-musa-green" alt="Torch"/>
  <img src="https://img.shields.io/badge/docker-ready-blue" alt="Docker"/>
</p>

> åŸºäº [LLaMA-Factory0.9.2](https://github.com/hiyouga/LLaMA-Factory) çš„é­”æ”¹ç‰ˆæœ¬ï¼Œé€‚é… **æ‘©å°”çº¿ç¨‹ S4000 GPU (MUSA æ¶æ„)**ï¼Œæ”¯æŒå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€å¾®è°ƒä¸æ¨ç†ï¼Œå¹¶æä¾› WebUI äº¤äº’ç•Œé¢ã€‚

---

## âœ¨ ç‰¹æ€§

- ğŸ”§ **MUSA é€‚é…**ï¼šæ”¯æŒ `torch_musa`ï¼Œå¯ç›´æ¥è¿è¡Œåœ¨æ‘©å°”çº¿ç¨‹ S4000 ä¸Š  
- ğŸ³ **Docker ä¸€é”®éƒ¨ç½²**ï¼šæä¾›å®˜æ–¹é•œåƒä¸æ„å»ºè„šæœ¬ï¼Œç¯å¢ƒå¯å¿«é€Ÿå¯åŠ¨  
- ğŸŒ **å›½å†…æºæ¨¡å‹ä¸‹è½½**ï¼šå·²åˆ‡æ¢ä¸º Modelscope é€šé“  
- âš¡ **æ€§èƒ½ä¼˜åŒ–**ï¼šæ˜¾å­˜ç¯å¢ƒå˜é‡é…ç½®ã€æºç å…¼å®¹æ€§è¡¥ä¸  
- ğŸ§© **æ¨¡å‹é€‚é…**ï¼šå·²æµ‹è¯•å¹¶ä¿®å¤ **Qwen2.5-3B** ç­‰æ¨¡å‹


## ğŸ“¦ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ ç¯å¢ƒè¦æ±‚
- Ubuntu 22.04.x LTS x86_64  
- Kernel == 5.15.0  
- æ‘©å°”çº¿ç¨‹ S4000 GPU  
- Docker / Docker Compose  

---

### 2ï¸âƒ£ å®‰è£… MUSA é©±åŠ¨
ğŸ‘‰ [å®˜æ–¹æ–‡æ¡£å‚è€ƒ](https://mcconline.mthreads.com/repo/musa-pytorch-release-public?repoName=musa-pytorch-release-public&repoNamespace=mcconline&displayName=Pytorch%20on%20MUSA%20Release)

---

### 3ï¸âƒ£ æ‹‰å–é•œåƒ
```bash
docker pull musa-pytorch-dev-public:rc3.1.0-v1.3.0-S4000-py310
```

---

### 4ï¸âƒ£ ä¸‹è½½æºç 
```bash
cd /data
git clone https://github.com/DTCTAI/LLaMA-Factory-0.9.2-S4000-musa.git
```

---

### 5ï¸âƒ£ æ„å»ºé•œåƒ
```bash
cd /data/LLaMA-Factory-0.9.2-S4000-musa/docker/docker-musa
docker build -t musa_s4000_llama_factory:0.1 .
```

---

### 6ï¸âƒ£ å¯åŠ¨æœåŠ¡
```bash
docker-compose up -d
```

---

### 7ï¸âƒ£ è¿›å…¥å®¹å™¨ & å¯åŠ¨ WebUI
```bash
docker exec -it [å®¹å™¨id] bash
cd /data/LLaMA-Factory-0.9.2-S4000-musa
pip install -e ".[torch,metrics]"
llamafactory-cli webui
```

è®¿é—®åœ°å€ï¼š  
ğŸ‘‰ `http://server_ip:7860`
---

## ğŸ”§ é­”æ”¹è¯´æ˜

### âœ… æºç ä¿®æ”¹
- `src/llamafactory/extras/misc.py`  
  æ–°å¢ `is_torch_musa_available` åˆ†æ”¯
- 
- æ‰€æœ‰çš„pyæ–‡ä»¶æ ¹æ®æ¡ä»¶å¢åŠ ï¼š
  ```å¦‚æœå­˜åœ¨ import torch, åˆ™åœ¨å…¶åå¢åŠ ï¼š
  import torch_musa
  ```

---

### âœ… ä¾èµ–æ›´æ–°
```bash
pip install --upgrade transformers
pip install pydantic==2.10.6
```

---

### âœ… æ¨¡å‹ä¸‹è½½ä¼˜åŒ–
æ”¯æŒ **Modelscope**ï¼Œé¿å…ç½‘ç»œå—é™ï¼š
```bash
export USE_MODELSCOPE_HUB=1
export MODELSCOPE_CACHE=/data/LLaMA-Factory-0.9.2-S4000-musa/Models
```

---

### âœ… æ˜¾å­˜æŠ¥é”™ä¿®å¤
å®¹å™¨å†…è®¾ç½®ï¼š
```bash
export MTHREADS_VISIBLE_DEVICES=1
```

---

### âœ… ç‰¹æ®Šæ¨¡å‹é€‚é…ï¼ˆQWen2.5_3Bï¼‰
ä¿®æ”¹ `transformers/integrations/sdpa_attention.py` ç¬¬ 53 è¡Œï¼š  
```python
scale = 1.0 / (query.size(-1) ** 0.5)
attn_output = F.scaled_dot_product_attention(
    query, key, value,
    attn_mask=attn_mask,
    dropout_p=dropout_p,
    is_causal=is_causal,
    scale=scale,
)
```

---

## ğŸ“Œ é¡¹ç›®ç›®æ ‡
- æ¨åŠ¨ **å›½äº§ GPU** åœ¨å¤§æ¨¡å‹é¢†åŸŸçš„åº”ç”¨è½åœ°  
- æ‰“é€š **LLaMA-Factory** ä¸ **MUSA ç”Ÿæ€** çš„å…¼å®¹æ€§  
- ä¸ºç§‘ç ”ä¸ä¼ä¸šæä¾› **ä½æˆæœ¬çš„å›½äº§ç®—åŠ›æ›¿ä»£æ–¹æ¡ˆ**  

---

## ğŸ“Š é¡¹ç›®ç»“æ„
```
LLaMA-Factory-0.9.2-S4000-musa
â”œâ”€â”€ docker/                # Docker æ„å»ºä¸è¿è¡Œé…ç½®
â”œâ”€â”€ src/llamafactory/      # æºç ä¿®æ”¹ä½ç½®
â”œâ”€â”€ models/                # æ¨¡å‹å­˜æ”¾ç›®å½•
â””â”€â”€ README.md              # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

---

## ğŸ¤ è´¡çŒ®
æ¬¢è¿æäº¤ PR æˆ– issueï¼Œä¸€èµ·å®Œå–„é€‚é…ä¸ä¼˜åŒ–ï¼  

---

## ğŸ“œ License
æœ¬é¡¹ç›®åŸºäº [Apache 2.0 License](./LICENSE) å¼€æºã€‚  
åŸé¡¹ç›®ç‰ˆæƒå½’ [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) æ‰€æœ‰ã€‚  

## è”ç³»æ–¹å¼
- å…¬å¸å®˜ç½‘: [https://www.dtctcn.com/](https://www.dtctcn.com/)
- äº§å“æœåŠ¡ç”µè¯: 400-865-5169